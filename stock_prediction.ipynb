{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stock_prediction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0vW8dqcWPWIq7GqNs/U54",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReutFarkash/stocks/blob/master/stock_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhUGa2ImH-FP",
        "colab_type": "text"
      },
      "source": [
        "https://www.thepythoncode.com/article/stock-price-prediction-in-python-using-tensorflow-2-and-keras\n",
        "How to Predict Stock Prices in Python using TensorFlow 2 and Keras\n",
        "Predicting different stock prices using Long Short-Term Memory Recurrent Neural Network in Python using TensorFlow 2 and Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgC3L4tXHsZ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e393ba59-1137-4a53-b2c1-539415b5dc2d"
      },
      "source": [
        "!pip3 install tensorflow pandas numpy matplotlib yahoo_fin sklearn requests_html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Collecting yahoo_fin\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/5c/6bf0c0147cc94d643e2a2413d0a9b27967e964ee99f88f26db93a0b963b8/yahoo_fin-0.8.6-py3-none-any.whl\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Collecting requests_html\n",
            "  Downloading https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.31.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Collecting pyppeteer>=0.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/4b/3c2aabdd1b91fa52aa9de6cde33b488b0592b4d48efb0ad9efbf71c49f5b/pyppeteer-0.2.2-py3-none-any.whl (145kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from requests_html) (0.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from requests_html) (2.23.0)\n",
            "Collecting w3lib\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Collecting parse\n",
            "  Downloading https://files.pythonhosted.org/packages/23/ea/ba7f9a5d728f7c45b298ae007b06908a5909eb5693a559ee5d70e7f84f92/parse-1.17.0.tar.gz\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting pyquery\n",
            "  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (49.6.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.16.0)\n",
            "Collecting pyee<8.0.0,>=7.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/41/79/3ec03b964c6e787ede1c34f29e5ead3f440965696ae0b3b0667f617deb75/pyee-7.0.3-py2.py3-none-any.whl\n",
            "Collecting tqdm<5.0.0,>=4.42.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.7MB/s \n",
            "\u001b[?25hCollecting appdirs<2.0.0,>=1.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting websockets<9.0,>=8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.2MB/s \n",
            "\u001b[?25hCollecting urllib3<2.0.0,>=1.25.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->requests_html) (4.6.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (3.0.4)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.6/dist-packages (from pyquery->requests_html) (4.2.6)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Building wheels for collected packages: parse, fake-useragent\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.17.0-cp36-none-any.whl size=24118 sha256=0aa1dd90bd56bb1ba41e079b46c8e644c820b2da75d0f5eef1ded85993bd2c5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/ad/41/87f17d17fdc2b5f9648e2ec2f9dbe6ad51c2f58f086baafedf\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=dd3c4a5d61c6781a8433527be2b6f4e317f53404395aa683e9c58a180e019fe9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "Successfully built parse fake-useragent\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: yahoo-fin, pyee, tqdm, appdirs, websockets, urllib3, pyppeteer, w3lib, parse, fake-useragent, cssselect, pyquery, requests-html\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed appdirs-1.4.4 cssselect-1.1.0 fake-useragent-0.1.11 parse-1.17.0 pyee-7.0.3 pyppeteer-0.2.2 pyquery-1.4.1 requests-html-0.10.0 tqdm-4.48.2 urllib3-1.25.10 w3lib-1.22.0 websockets-8.1 yahoo-fin-0.8.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIBaeIbGIDlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from yahoo_fin import stock_info as si\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqb7QhuoITpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set seed, so we can get the same results after rerunning several times\n",
        "np.random.seed(314)\n",
        "tf.random.set_seed(314)\n",
        "random.seed(314)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61NhYlGcIf0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, \n",
        "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
        "    \"\"\"\n",
        "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
        "    Params:\n",
        "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
        "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
        "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
        "        shuffle (bool): whether to shuffle the data, default is True\n",
        "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
        "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
        "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
        "    \"\"\"\n",
        "    # see if ticker is already a loaded stock from yahoo finance\n",
        "    if isinstance(ticker, str):\n",
        "        # load it from yahoo_fin library\n",
        "        df = si.get_data(ticker)\n",
        "    elif isinstance(ticker, pd.DataFrame):\n",
        "        # already loaded, use it directly\n",
        "        df = ticker\n",
        "    else:\n",
        "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
        "    # this will contain all the elements we want to return from this function\n",
        "    result = {}\n",
        "    # we will also return the original dataframe itself\n",
        "    result['df'] = df.copy()\n",
        "    # make sure that the passed feature_columns exist in the dataframe\n",
        "    for col in feature_columns:\n",
        "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
        "    if scale:\n",
        "        column_scaler = {}\n",
        "        # scale the data (prices) from 0 to 1\n",
        "        for column in feature_columns:\n",
        "            scaler = preprocessing.MinMaxScaler()\n",
        "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
        "            column_scaler[column] = scaler\n",
        "        # add the MinMaxScaler instances to the result returned\n",
        "        result[\"column_scaler\"] = column_scaler\n",
        "    # add the target column (label) by shifting by `lookup_step`\n",
        "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
        "    # last `lookup_step` columns contains NaN in future column\n",
        "    # get them before droping NaNs\n",
        "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
        "    # drop NaNs\n",
        "    df.dropna(inplace=True)\n",
        "    sequence_data = []\n",
        "    sequences = deque(maxlen=n_steps)\n",
        "    for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
        "        sequences.append(entry)\n",
        "        if len(sequences) == n_steps:\n",
        "            sequence_data.append([np.array(sequences), target])\n",
        "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
        "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
        "    # this last_sequence will be used to predict future stock prices not available in the dataset\n",
        "    last_sequence = list(sequences) + list(last_sequence)\n",
        "    last_sequence = np.array(last_sequence)\n",
        "    # add to result\n",
        "    result['last_sequence'] = last_sequence\n",
        "    # construct the X's and y's\n",
        "    X, y = [], []\n",
        "    for seq, target in sequence_data:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "    # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    # reshape X to fit the neural network\n",
        "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
        "    # split the dataset\n",
        "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
        "                                                                               test_size=test_size, shuffle=shuffle)\n",
        "    # return the result\n",
        "    return result"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym63baQlIgFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
        "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
        "    model = Sequential()\n",
        "    for i in range(n_layers):\n",
        "        if i == 0:\n",
        "            # first layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\n",
        "        elif i == n_layers - 1:\n",
        "            # last layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=False))\n",
        "        else:\n",
        "            # hidden layers\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True))\n",
        "        # add dropout after each layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O3Sr0ACIg0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Window size or the sequence length\n",
        "N_STEPS = 70\n",
        "# Lookup step, 1 is the next day\n",
        "LOOKUP_STEP = 1\n",
        "# test ratio size, 0.2 is 20%\n",
        "TEST_SIZE = 0.2\n",
        "# features to use\n",
        "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
        "# date now\n",
        "date_now = time.strftime(\"%Y-%m-%d\")\n",
        "### model parameters\n",
        "N_LAYERS = 3\n",
        "# LSTM cell\n",
        "CELL = LSTM\n",
        "# 256 LSTM neurons\n",
        "UNITS = 256\n",
        "# 40% dropout\n",
        "DROPOUT = 0.4\n",
        "# whether to use bidirectional RNNs\n",
        "BIDIRECTIONAL = False\n",
        "### training parameters\n",
        "# mean absolute error loss\n",
        "# LOSS = \"mae\"\n",
        "# huber loss\n",
        "LOSS = \"huber_loss\"\n",
        "OPTIMIZER = \"adam\"\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "# Tesla stock market\n",
        "ticker = \"TSLA\"\n",
        "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
        "# model name to save, making it as unique as possible based on parameters\n",
        "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
        "if BIDIRECTIONAL:\n",
        "    model_name += \"-b\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wCfD5gzIhAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create these folders if they does not exist\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "if not os.path.isdir(\"data\"):\n",
        "    os.mkdir(\"data\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wf8vzdbIhIY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e210ee83-b33b-4cc4-c373-9b942ea485d3"
      },
      "source": [
        "# load the data\n",
        "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
        "# save the dataframe\n",
        "data[\"df\"].to_csv(ticker_data_filename)\n",
        "# construct the model\n",
        "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
        "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
        "# some tensorflow callbacks\n",
        "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
        "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
        "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
        "                    callbacks=[checkpointer, tensorboard],\n",
        "                    verbose=1)\n",
        "model.save(os.path.join(\"results\", model_name) + \".h5\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.0082 - mean_absolute_error: 0.0960WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            " 2/32 [>.............................] - ETA: 1s - loss: 0.0064 - mean_absolute_error: 0.0666WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0197s vs `on_train_batch_end` time: 0.0837s). Check your callbacks.\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 0.0015 - mean_absolute_error: 0.0335\n",
            "Epoch 00001: val_loss improved from inf to 0.00038, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 2s 57ms/step - loss: 0.0014 - mean_absolute_error: 0.0321 - val_loss: 3.8406e-04 - val_mean_absolute_error: 0.0194\n",
            "Epoch 2/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 5.3889e-04 - mean_absolute_error: 0.0191\n",
            "Epoch 00002: val_loss improved from 0.00038 to 0.00033, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 5.3279e-04 - mean_absolute_error: 0.0190 - val_loss: 3.3138e-04 - val_mean_absolute_error: 0.0161\n",
            "Epoch 3/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 4.9015e-04 - mean_absolute_error: 0.0184\n",
            "Epoch 00003: val_loss improved from 0.00033 to 0.00033, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 4.7606e-04 - mean_absolute_error: 0.0181 - val_loss: 3.2711e-04 - val_mean_absolute_error: 0.0128\n",
            "Epoch 4/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 4.8187e-04 - mean_absolute_error: 0.0188\n",
            "Epoch 00004: val_loss did not improve from 0.00033\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 4.8187e-04 - mean_absolute_error: 0.0188 - val_loss: 3.8257e-04 - val_mean_absolute_error: 0.0147\n",
            "Epoch 5/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 5.1714e-04 - mean_absolute_error: 0.0172\n",
            "Epoch 00005: val_loss did not improve from 0.00033\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 5.1714e-04 - mean_absolute_error: 0.0172 - val_loss: 4.9426e-04 - val_mean_absolute_error: 0.0202\n",
            "Epoch 6/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 6.3435e-04 - mean_absolute_error: 0.0213\n",
            "Epoch 00006: val_loss improved from 0.00033 to 0.00022, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 6.1151e-04 - mean_absolute_error: 0.0208 - val_loss: 2.2411e-04 - val_mean_absolute_error: 0.0116\n",
            "Epoch 7/50\n",
            "31/32 [============================>.] - ETA: 0s - loss: 5.0659e-04 - mean_absolute_error: 0.0175\n",
            "Epoch 00007: val_loss did not improve from 0.00022\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 5.0456e-04 - mean_absolute_error: 0.0175 - val_loss: 2.6909e-04 - val_mean_absolute_error: 0.0144\n",
            "Epoch 8/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 5.9036e-04 - mean_absolute_error: 0.0210\n",
            "Epoch 00008: val_loss improved from 0.00022 to 0.00021, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 5.9787e-04 - mean_absolute_error: 0.0208 - val_loss: 2.0940e-04 - val_mean_absolute_error: 0.0123\n",
            "Epoch 9/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 4.8412e-04 - mean_absolute_error: 0.0174\n",
            "Epoch 00009: val_loss did not improve from 0.00021\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 4.8412e-04 - mean_absolute_error: 0.0174 - val_loss: 4.9082e-04 - val_mean_absolute_error: 0.0218\n",
            "Epoch 10/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 6.6728e-04 - mean_absolute_error: 0.0212\n",
            "Epoch 00010: val_loss did not improve from 0.00021\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 6.5270e-04 - mean_absolute_error: 0.0211 - val_loss: 3.0095e-04 - val_mean_absolute_error: 0.0164\n",
            "Epoch 11/50\n",
            "30/32 [===========================>..] - ETA: 0s - loss: 5.2477e-04 - mean_absolute_error: 0.0188\n",
            "Epoch 00011: val_loss did not improve from 0.00021\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 5.2942e-04 - mean_absolute_error: 0.0189 - val_loss: 2.3411e-04 - val_mean_absolute_error: 0.0127\n",
            "Epoch 12/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 4.9773e-04 - mean_absolute_error: 0.0180\n",
            "Epoch 00012: val_loss did not improve from 0.00021\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 4.9155e-04 - mean_absolute_error: 0.0180 - val_loss: 2.6842e-04 - val_mean_absolute_error: 0.0154\n",
            "Epoch 13/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 3.7762e-04 - mean_absolute_error: 0.0160\n",
            "Epoch 00013: val_loss did not improve from 0.00021\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 3.7762e-04 - mean_absolute_error: 0.0160 - val_loss: 3.1449e-04 - val_mean_absolute_error: 0.0150\n",
            "Epoch 14/50\n",
            "31/32 [============================>.] - ETA: 0s - loss: 3.4907e-04 - mean_absolute_error: 0.0160\n",
            "Epoch 00014: val_loss improved from 0.00021 to 0.00020, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 3.4768e-04 - mean_absolute_error: 0.0159 - val_loss: 2.0026e-04 - val_mean_absolute_error: 0.0119\n",
            "Epoch 15/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 3.1913e-04 - mean_absolute_error: 0.0135\n",
            "Epoch 00015: val_loss did not improve from 0.00020\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 3.1913e-04 - mean_absolute_error: 0.0135 - val_loss: 2.1622e-04 - val_mean_absolute_error: 0.0108\n",
            "Epoch 16/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 3.8387e-04 - mean_absolute_error: 0.0158\n",
            "Epoch 00016: val_loss improved from 0.00020 to 0.00016, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 3.8387e-04 - mean_absolute_error: 0.0158 - val_loss: 1.5530e-04 - val_mean_absolute_error: 0.0106\n",
            "Epoch 17/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 3.3078e-04 - mean_absolute_error: 0.0156\n",
            "Epoch 00017: val_loss did not improve from 0.00016\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 3.3078e-04 - mean_absolute_error: 0.0156 - val_loss: 1.6441e-04 - val_mean_absolute_error: 0.0119\n",
            "Epoch 18/50\n",
            "30/32 [===========================>..] - ETA: 0s - loss: 4.8082e-04 - mean_absolute_error: 0.0168\n",
            "Epoch 00018: val_loss did not improve from 0.00016\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 4.8441e-04 - mean_absolute_error: 0.0172 - val_loss: 5.4018e-04 - val_mean_absolute_error: 0.0260\n",
            "Epoch 19/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 3.6643e-04 - mean_absolute_error: 0.0154\n",
            "Epoch 00019: val_loss did not improve from 0.00016\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 3.4996e-04 - mean_absolute_error: 0.0151 - val_loss: 1.9107e-04 - val_mean_absolute_error: 0.0111\n",
            "Epoch 20/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 2.8028e-04 - mean_absolute_error: 0.0129\n",
            "Epoch 00020: val_loss improved from 0.00016 to 0.00012, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 2.8028e-04 - mean_absolute_error: 0.0129 - val_loss: 1.2290e-04 - val_mean_absolute_error: 0.0086\n",
            "Epoch 21/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 2.9224e-04 - mean_absolute_error: 0.0132\n",
            "Epoch 00021: val_loss improved from 0.00012 to 0.00010, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 2.8921e-04 - mean_absolute_error: 0.0132 - val_loss: 1.0033e-04 - val_mean_absolute_error: 0.0077\n",
            "Epoch 22/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 2.5544e-04 - mean_absolute_error: 0.0124\n",
            "Epoch 00022: val_loss improved from 0.00010 to 0.00009, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 2.5843e-04 - mean_absolute_error: 0.0125 - val_loss: 8.7299e-05 - val_mean_absolute_error: 0.0082\n",
            "Epoch 23/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 4.0916e-04 - mean_absolute_error: 0.0164\n",
            "Epoch 00023: val_loss improved from 0.00009 to 0.00007, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 3.8818e-04 - mean_absolute_error: 0.0160 - val_loss: 7.3753e-05 - val_mean_absolute_error: 0.0069\n",
            "Epoch 24/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 2.1911e-04 - mean_absolute_error: 0.0121\n",
            "Epoch 00024: val_loss did not improve from 0.00007\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 2.2051e-04 - mean_absolute_error: 0.0120 - val_loss: 7.6877e-05 - val_mean_absolute_error: 0.0067\n",
            "Epoch 25/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 2.8691e-04 - mean_absolute_error: 0.0143\n",
            "Epoch 00025: val_loss did not improve from 0.00007\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 2.8691e-04 - mean_absolute_error: 0.0143 - val_loss: 2.3765e-04 - val_mean_absolute_error: 0.0174\n",
            "Epoch 26/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 2.3475e-04 - mean_absolute_error: 0.0127\n",
            "Epoch 00026: val_loss did not improve from 0.00007\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 2.3530e-04 - mean_absolute_error: 0.0129 - val_loss: 8.2209e-05 - val_mean_absolute_error: 0.0084\n",
            "Epoch 27/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 2.6487e-04 - mean_absolute_error: 0.0127\n",
            "Epoch 00027: val_loss did not improve from 0.00007\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 2.6487e-04 - mean_absolute_error: 0.0127 - val_loss: 1.5983e-04 - val_mean_absolute_error: 0.0119\n",
            "Epoch 28/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 3.3441e-04 - mean_absolute_error: 0.0161\n",
            "Epoch 00028: val_loss improved from 0.00007 to 0.00005, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 19ms/step - loss: 3.2174e-04 - mean_absolute_error: 0.0157 - val_loss: 5.2477e-05 - val_mean_absolute_error: 0.0055\n",
            "Epoch 29/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 2.0405e-04 - mean_absolute_error: 0.0119\n",
            "Epoch 00029: val_loss did not improve from 0.00005\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 2.0405e-04 - mean_absolute_error: 0.0119 - val_loss: 6.2304e-05 - val_mean_absolute_error: 0.0073\n",
            "Epoch 30/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 2.3594e-04 - mean_absolute_error: 0.0130\n",
            "Epoch 00030: val_loss did not improve from 0.00005\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 2.3594e-04 - mean_absolute_error: 0.0130 - val_loss: 7.1453e-05 - val_mean_absolute_error: 0.0075\n",
            "Epoch 31/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 1.9855e-04 - mean_absolute_error: 0.0121\n",
            "Epoch 00031: val_loss did not improve from 0.00005\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.9067e-04 - mean_absolute_error: 0.0119 - val_loss: 7.1287e-05 - val_mean_absolute_error: 0.0081\n",
            "Epoch 32/50\n",
            "31/32 [============================>.] - ETA: 0s - loss: 1.6153e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 00032: val_loss did not improve from 0.00005\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 1.6112e-04 - mean_absolute_error: 0.0104 - val_loss: 6.1036e-05 - val_mean_absolute_error: 0.0063\n",
            "Epoch 33/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 1.7438e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 00033: val_loss improved from 0.00005 to 0.00004, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 1.6963e-04 - mean_absolute_error: 0.0108 - val_loss: 4.0600e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 34/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 2.0229e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 00034: val_loss improved from 0.00004 to 0.00004, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 2.0229e-04 - mean_absolute_error: 0.0109 - val_loss: 3.9357e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 35/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 1.3286e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00035: val_loss improved from 0.00004 to 0.00004, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.2915e-04 - mean_absolute_error: 0.0094 - val_loss: 3.7581e-05 - val_mean_absolute_error: 0.0057\n",
            "Epoch 36/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 1.6635e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 00036: val_loss did not improve from 0.00004\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.6351e-04 - mean_absolute_error: 0.0105 - val_loss: 7.3843e-05 - val_mean_absolute_error: 0.0097\n",
            "Epoch 37/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 1.3295e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00037: val_loss improved from 0.00004 to 0.00003, saving model to results/2020-09-05_TSLA-huber_loss-adam-LSTM-seq-70-step-1-layers-3-units-256.h5\n",
            "32/32 [==============================] - 1s 20ms/step - loss: 1.3874e-04 - mean_absolute_error: 0.0094 - val_loss: 2.9480e-05 - val_mean_absolute_error: 0.0037\n",
            "Epoch 38/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 1.7512e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 00038: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.7066e-04 - mean_absolute_error: 0.0103 - val_loss: 3.0039e-05 - val_mean_absolute_error: 0.0048\n",
            "Epoch 39/50\n",
            "31/32 [============================>.] - ETA: 0s - loss: 1.6050e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00039: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 1.5989e-04 - mean_absolute_error: 0.0095 - val_loss: 4.6800e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 40/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 1.2916e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00040: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.2838e-04 - mean_absolute_error: 0.0095 - val_loss: 2.9496e-05 - val_mean_absolute_error: 0.0038\n",
            "Epoch 41/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 1.2564e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00041: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.2564e-04 - mean_absolute_error: 0.0089 - val_loss: 3.4442e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 42/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 1.4689e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00042: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.5352e-04 - mean_absolute_error: 0.0097 - val_loss: 3.6475e-05 - val_mean_absolute_error: 0.0055\n",
            "Epoch 43/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 1.2948e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00043: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.2948e-04 - mean_absolute_error: 0.0095 - val_loss: 2.9497e-05 - val_mean_absolute_error: 0.0039\n",
            "Epoch 44/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 1.3907e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00044: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.3818e-04 - mean_absolute_error: 0.0098 - val_loss: 9.2717e-05 - val_mean_absolute_error: 0.0107\n",
            "Epoch 45/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 1.3944e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 00045: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.3944e-04 - mean_absolute_error: 0.0104 - val_loss: 7.1121e-05 - val_mean_absolute_error: 0.0094\n",
            "Epoch 46/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 1.4655e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00046: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.4655e-04 - mean_absolute_error: 0.0102 - val_loss: 5.3190e-05 - val_mean_absolute_error: 0.0055\n",
            "Epoch 47/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 1.5166e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00047: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.5379e-04 - mean_absolute_error: 0.0095 - val_loss: 3.1119e-05 - val_mean_absolute_error: 0.0047\n",
            "Epoch 48/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 1.6553e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 00048: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.7087e-04 - mean_absolute_error: 0.0111 - val_loss: 8.9681e-05 - val_mean_absolute_error: 0.0114\n",
            "Epoch 49/50\n",
            "29/32 [==========================>...] - ETA: 0s - loss: 1.2399e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00049: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.3760e-04 - mean_absolute_error: 0.0097 - val_loss: 7.1780e-05 - val_mean_absolute_error: 0.0087\n",
            "Epoch 50/50\n",
            "32/32 [==============================] - ETA: 0s - loss: 1.4917e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 00050: val_loss did not improve from 0.00003\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 1.4917e-04 - mean_absolute_error: 0.0110 - val_loss: 3.3539e-05 - val_mean_absolute_error: 0.0045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqsXVKByIhL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tensorboard --logdir=\"logs\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtuACy80Ifls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
        "                feature_columns=FEATURE_COLUMNS, shuffle=False)\n",
        "# construct the model\n",
        "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
        "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
        "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
        "model.load_weights(model_path)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WktsruRJEqE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba6c1196-f5d8-4fd2-94fc-2e5080cc92fe"
      },
      "source": [
        "# evaluate the model\n",
        "mse, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
        "# calculate the mean absolute error (inverse scaling)\n",
        "mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 8.358148490927634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8bOVCc8JEtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, data):\n",
        "    # retrieve the last sequence from data\n",
        "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
        "    # retrieve the column scalers\n",
        "    column_scaler = data[\"column_scaler\"]\n",
        "    # reshape the last sequence\n",
        "    last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\n",
        "    # expand dimension\n",
        "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "    # get the prediction (scaled from 0 to 1)\n",
        "    prediction = model.predict(last_sequence)\n",
        "    # get the price (by inverting the scaling)\n",
        "    predicted_price = column_scaler[\"adjclose\"].inverse_transform(prediction)[0][0]\n",
        "    return predicted_price"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjpaTfziJEwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "adf91b50-30a6-4cef-804c-8e3ca7e5d71c"
      },
      "source": [
        "# predict the future price\n",
        "future_price = predict(model, data)\n",
        "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Future price after 1 days is 418.10$\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNL7Uw3yJEzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_graph(model, data):\n",
        "    y_test = data[\"y_test\"]\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "    # last 200 days, feel free to edit that\n",
        "    plt.plot(y_test[-200:], c='b')\n",
        "    plt.plot(y_pred[-200:], c='r')\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
        "    plt.show()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXOZJLGdJE2S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "592e19e2-9a54-43cf-c97c-80a367feb243"
      },
      "source": [
        "plot_graph(model, data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GkuKm1JJRK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(model, data):\n",
        "    y_test = data[\"y_test\"]\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "    y_pred = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_pred[LOOKUP_STEP:]))\n",
        "    y_test = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_test[LOOKUP_STEP:]))\n",
        "    return accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eftki4C0JRNz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85cf5ada-4e3d-4906-d6d5-3cea521c1cc5"
      },
      "source": [
        "print(str(LOOKUP_STEP) + \":\", \"Accuracy Score:\", get_accuracy(model, data))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: Accuracy Score: 0.5190380761523046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFurwh0YJRQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh9QJm_ZJRTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llp_ZvUWJRWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}